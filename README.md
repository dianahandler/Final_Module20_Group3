# Machine Learning w/ Sklearn
We would like to predict Austin house prices with a linear regression model.

## Correlation Analysis
With over 40 input features related to each Austin home, we decided to conduct correlation analysis to help determine the importance of each feature to home prices. Following this process, we found that the top 3 most correlated factors – median_zip, zip_rank, and pr_sqft (in no particular order) – were based off of 'latestPrice' figures, the output of our project. Boasting correlation coefficients between about 0.576 to about 0.731, these features showed moderate to strong correlation to 'latestPrice'. While these 3 features are dependent on 'latestPrice', we nevertheless feel that they may serve as key factors to the house's final selling price, whether it's because some zip codes are attractive to prospective homebuyers or if because some homebuyers want the most living space within their budget. Aside from these 3 features, the rest of the inputs' correlation coefficients fall below 0.5, suggesting weak at best correlation. Nevertheless, since a wide majority of features aren't considered "moderately" correlated to house price, we will include as many weakly correlated features as possible. For this demonstration in particular, **we opted to use the top 10 correlated features**, with the least correlated feature having a coefficient of about 0.209.

## Multilinear Regression
With our features and output defined, we used Sklearn's linear regression function first mentioned in 17.2.3 (2021). Unlike in 17.2.3, however, we'll feed multiple dimensions of features into the regression model so that it would predict house prices. More specifically, we'll take a page out of 17.3.1 (2021) and split our data into training and testing sets. While we could feed the machine learning model all 12,000+ rows of Austin housing data, as noted in 17.3.1 (2021), we want our model to make accurate predictions in the face of new, real-world data. For analyzing our model's performance, we opted to calculate root mean squared error and mean absolute error for initial metrics. Visually, we opted to plot predicted values against actual test results on a 2-d graph. On the same graph, we plotted a straight line with slope 1 that passes through the origin so that we can gauge the model's overpredictions and underpredictions.
